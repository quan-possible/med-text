{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base_classifier import BaseClassifier\n",
    "from multilabel import MultiLabelClassifier\n",
    "from multiclass import MultiClassClassifier\n",
    "from tokenizer import Tokenizer\n",
    "from datamodule import MedDataModule, Collator\n",
    "\n",
    "import pathlib\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from sys import path\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser, Namespace\n",
    "from collections import OrderedDict\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning.utilities.seed import seed_everything\n",
    "from torchnlp.random import set_seed\n",
    "from torchmetrics.functional import f1, precision_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hparams(experiment_dir: str):\n",
    "    hparams_file = experiment_dir + \"/hparams.yaml\"\n",
    "    hparams = yaml.load(open(hparams_file).read(), Loader=yaml.FullLoader)\n",
    "    # print(Namespace(**hparams))\n",
    "    return Namespace(**hparams)\n",
    "\n",
    "\n",
    "def load_model(experiment_dir: str, dataset, hparams, tokenizer, collator, num_classes):\n",
    "    \"\"\" Function that loads the model from an experiment folder.\n",
    "    :param experiment_dir: Path to the experiment folder.\n",
    "    Return:\n",
    "        - Pretrained model.\n",
    "    \"\"\"\n",
    "    experiment_path = Path(experiment_dir + \"/checkpoints/\")\n",
    "    \n",
    "    # hparams_file = experiment_dir + \"/hparams.yaml\"\n",
    "    # hparams = dotdict(yaml.load(open(hparams_file).read(), Loader=yaml.FullLoader))\n",
    "\n",
    "    checkpoints = [\n",
    "        file.name\n",
    "        for file in experiment_path.iterdir()\n",
    "        if file.name.endswith(\".ckpt\")\n",
    "    ]\n",
    "    checkpoint_path = experiment_path / checkpoints[-1]\n",
    "    \n",
    "    classifier = MultiLabelClassifier if dataset == \"hoc\" else MultiClassClassifier\n",
    "    \n",
    "    model = classifier.load_from_checkpoint(\n",
    "        checkpoint_path, hparams=hparams, tokenizer=tokenizer,\n",
    "        collator=collator, encoder_model=hparams.encoder_model,\n",
    "        batch_size=hparams.batch_size,\n",
    "        num_frozen_epochs=hparams.num_frozen_epochs,\n",
    "        #  label_encoder,\n",
    "        encoder_learning_rate=hparams.encoder_learning_rate, \n",
    "        learning_rate=hparams.learning_rate,\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Make sure model is in prediction mode\n",
    "    model.eval()\n",
    "    model.freeze()\n",
    "    return model\n",
    "\n",
    "def prototype(hparams):\n",
    "    seed_everything(69)\n",
    "    \n",
    "    tokenizer = Tokenizer(hparams.encoder_model)\n",
    "    collator = Collator(tokenizer)\n",
    "    datamodule = MedDataModule(\n",
    "        tokenizer, collator, hparams.data_path,\n",
    "        hparams.dataset, hparams.batch_size, \n",
    "        hparams.num_workers,\n",
    "    )\n",
    "    \n",
    "    desc_tokens = datamodule.desc_tokens\n",
    "    num_classes = datamodule.num_classes\n",
    "    train_size = datamodule.size(dim=0)\n",
    "    print(\"Finished loading data!\")\n",
    "    \n",
    "    \n",
    "    if hparams.dataset == 'hoc':\n",
    "        model = MultiLabelClassifier(\n",
    "            desc_tokens, tokenizer, collator, num_classes, train_size, hparams, **vars(hparams)\n",
    "        )\n",
    "    else:\n",
    "        model = MultiClassClassifier(\n",
    "            desc_tokens, tokenizer, collator, num_classes, train_size, hparams, **vars(hparams)\n",
    "        )\n",
    "        \n",
    "    return model, datamodule, hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-f65413967027>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-f65413967027>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    hparams =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "hparams = Namespace(\n",
    "    encoder_model=\"bert-base-cased\",\n",
    "    data_path=\"./project/data\",\n",
    "    dataset=\"mtc\",\n",
    "    batch_size=2,\n",
    "    num_workers=2,\n",
    "    random_sampling=False,\n",
    "    num_frozen_epochs=0,\n",
    "    encoder_learning_rate=1e-05,\n",
    "    learning_rate=3e-05,\n",
    "    tgt_txt_col=\"TEXT\",\n",
    "    tgt_lbl_col=\"LABEL\",\n",
    "    n_lbl_attn_layer=1,\n",
    "    static_desc_emb=True,\n",
    "    weight_decay_encoder=0.05,\n",
    "    weight_decay_nonencoder=0.1,\n",
    "    label_attn_lr=0.0002,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
