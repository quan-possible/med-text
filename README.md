---

<div align="center">    
 
# Your Project Name     

[![Paper](http://img.shields.io/badge/paper-arxiv.1001.2234-B31B1B.svg)](https://arxiv.org/abs/2108.11809)

ARXIV   
[![Paper](http://img.shields.io/badge/arxiv-math.co:1480.1111-B31B1B.svg)](https://arxiv.org/abs/2108.11809)

</div>
 
## Description   
This project proposes a novel fine-tuning scheme for pretrained language models. The approach involves adding a label attention module on top of a pretrained BERT, thereby injecting label information into the fine-tuning process, as well as rendering the classification more explainable

### Citation   
```
@article{nguyen2021fine,
  title={Fine-tuning Pretrained Language Models with Label Attention for Explainable Biomedical Text Classification},
  author={Nguyen, Bruce and Ji, Shaoxiong},
  journal={arXiv preprint arXiv:2108.11809},
  year={2021}
}
```   
